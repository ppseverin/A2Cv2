{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import  as tf\n",
    "\n",
    "# import gym\n",
    "# import gym_anytrading\n",
    "\n",
    "# import matplotlib.animation as animation \n",
    "\n",
    "from keras.models import Model,load_model,save_model\n",
    "from keras.layers import Dense, Input,Dropout,LSTM\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# from keras import layers, models,\n",
    "from keras import backend as K\n",
    "\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import talib\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \"\"\"Sets up a memory replay for actor-critic training.\n",
    "\n",
    "    Args:\n",
    "        gamma (float): The \"discount rate\" used to assess state values.\n",
    "        batch_size (int): The number of elements to include in the buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, batch_size):\n",
    "        self.buffer = []\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience into the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: (state, action, reward, state_prime_value, done) tuple.\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def check_full(self):\n",
    "        return len(self.buffer) >= self.batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Returns formated experiences and clears the buffer.\n",
    "\n",
    "        Returns:\n",
    "            (list): A tuple of lists with structure [\n",
    "                [states], [actions], [rewards], [state_prime_values], [dones]\n",
    "            ]\n",
    "        \"\"\"\n",
    "        # Columns have different data types, so numpy array would be awkward.\n",
    "        batch = np.array(self.buffer).T.tolist()\n",
    "\n",
    "\n",
    "        states_mb = np.array(batch[0], dtype=np.float32)\n",
    "\n",
    "        actions_mb = np.array(batch[1], dtype=np.int8)\n",
    "\n",
    "        rewards_mb = np.array(batch[2], dtype=np.float32)\n",
    "\n",
    "        dones_mb = np.array(batch[3], dtype=np.int8)\n",
    "        value_mb = np.squeeze(np.array(batch[4], dtype=np.float32))\n",
    "        self.buffer = []\n",
    "        return states_mb, actions_mb, rewards_mb, dones_mb, value_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Sets up a reinforcement learning agent to play in a game environment.\"\"\"\n",
    "    def __init__(self, actor, critic, policy, memory, action_size):\n",
    "        \"\"\"Initializes the agent with DQN and memory sub-classes.\n",
    "\n",
    "        Args:\n",
    "            network: A neural network created from deep_q_network().\n",
    "            memory: A Memory class object.\n",
    "            epsilon_decay (float): The rate at which to decay random actions.\n",
    "            action_size (int): The number of possible actions to take.\n",
    "        \"\"\"\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.policy = policy\n",
    "        self.action_size = action_size\n",
    "        self.memory = memory\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects an action for the agent to take given a game state.\n",
    "\n",
    "        Args:\n",
    "            state (list of numbers): The state of the environment to act on.\n",
    "            traning (bool): True if the agent is training.\n",
    "\n",
    "        Returns:\n",
    "            (int) The index of the action to take.\n",
    "        \"\"\"\n",
    "        # If not acting randomly, take action with highest predicted value.\n",
    "\n",
    "        if len(state.shape)<3:\n",
    "            state=np.reshape(state,(1,1,state.shape[1]))\n",
    "\n",
    "        probabilities = self.policy.predict(state)[0]\n",
    "\n",
    "        action = np.random.choice(self.action_size, p=probabilities)\n",
    "        return action\n",
    "\n",
    "    def learn(self, print_variables=False):\n",
    "        \"\"\"Trains the Deep Q Network based on stored experiences.\"\"\"\n",
    "        gamma = self.memory.gamma\n",
    "        experiences = self.memory.sample()\n",
    "        state_mb, action_mb, reward_mb, dones_mb, next_value = experiences\n",
    "\n",
    "        # One hot enocde actions\n",
    "        actions = np.zeros([len(action_mb), self.action_size])\n",
    "        actions[np.arange(len(action_mb)), action_mb] = 1\n",
    "\n",
    "        #Apply TD(0)\n",
    "        discount_mb = reward_mb + next_value * gamma * (1 - dones_mb)\n",
    "\n",
    "        state_values = self.critic.predict([state_mb])\n",
    "        advantages = discount_mb - np.squeeze(state_values)\n",
    "\n",
    "        if print_variables:\n",
    "            print(\"discount_mb\", discount_mb)\n",
    "            print(\"next_value\", next_value)\n",
    "            print(\"state_values\", state_values)\n",
    "            print(\"advantages\", advantages)\n",
    "        else:\n",
    "            self.actor.train_on_batch(\n",
    "                [state_mb, advantages], [actions, discount_mb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_networks(state_shape, action_size, learning_rate, critic_weight,\n",
    "                   hidden_neurons, entropy):\n",
    "    \"\"\"Creates Actor Critic Neural Networks.\n",
    "\n",
    "    Creates a two hidden-layer Policy Gradient Neural Network. The loss\n",
    "    function is altered to be a log-likelihood function weighted\n",
    "    by an action's advantage.\n",
    "\n",
    "    Args:\n",
    "        space_shape: a tuple of ints representing the observation space.\n",
    "        action_size (int): the number of possible actions.\n",
    "        learning_rate (float): the nueral network's learning rate.\n",
    "        critic_weight (float): how much to weigh the critic's training loss.\n",
    "        hidden_neurons (int): the number of neurons to use per hidden layer.\n",
    "        entropy (float): how much to enourage exploration versus exploitation.\n",
    "    \"\"\"\n",
    "    state_input = Input(state_shape, name='frames')\n",
    "    advantages = Input((1,), name='advantages')  # PG, A instead of G\n",
    "\n",
    "    # PG\n",
    "    actor_1 = LSTM(input_shape = (10,80),units=hidden_neurons, return_sequences = True)(state_input)\n",
    "    actor_2 = LSTM(input_shape = (10,80), units=hidden_neurons, return_sequences = False)(actor_1)\n",
    "    probabilities = Dense(action_size, activation='softmax')(actor_2)\n",
    "\n",
    "    # DQN\n",
    "    critic_1 = LSTM(input_shape = (10,80), units = hidden_neurons, return_sequences = True)(state_input)\n",
    "    critic_2 = LSTM(input_shape = (10,80), units=hidden_neurons, return_sequences = False)(critic_1)\n",
    "    values = Dense(1, activation='linear')(critic_2)\n",
    "\n",
    "    def actor_loss(y_true, y_pred):  # PG\n",
    "        y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1-CLIP_EDGE)\n",
    "        log_lik = y_true*K.log(y_pred_clipped)\n",
    "        entropy_loss = y_pred * K.log(K.clip(y_pred, CLIP_EDGE, 1-CLIP_EDGE))  # New\n",
    "        return K.sum(-log_lik * advantages) - (entropy * K.sum(entropy_loss))\n",
    "\n",
    "    # Train both actor and critic at the same time.\n",
    "    actor = Model(\n",
    "        inputs=[state_input, advantages], outputs=[probabilities, values])\n",
    "    actor.compile(\n",
    "        loss=[actor_loss, 'mean_squared_error'],  # [PG, DQN]\n",
    "        loss_weights=[1, critic_weight],  # [PG, DQN]\n",
    "        optimizer=Adam(lr=learning_rate))\n",
    "\n",
    "    critic = Model(inputs=[state_input], outputs=[values])\n",
    "    policy = Model(inputs=[state_input], outputs=[probabilities])\n",
    "    return actor, critic, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class currencyEnv:\n",
    "    \"\"\"\n",
    "    Enviroment de la moneda en cuestion.\n",
    "    State: vector de largo 3 (num_currencies * 2 + 1)\n",
    "     - # de posiciones abiertas (las dejaremos en 1 en todo momento)\n",
    "     - # precio observado de la moneda\n",
    "     - Saldo de la cuenta (o pips acumulados?)\n",
    "    Action: variable categorica con 3 posibilidades (3 a la potencia de la cantidad de monedas-3^currencies)\n",
    "     - 0 = Buy\n",
    "     - 1 = Sell\n",
    "     - 2 = Hold\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,data,initial_investment=20000):\n",
    "        #data\n",
    "        self.currency_price_history = data\n",
    "        self.n_step, self.n_features = self.currency_price_history.shape\n",
    "        self.n_features = self.n_features-1\n",
    "        self.n_currencies=1\n",
    "        self.op_buy = None\n",
    "        self.op_sell = None\n",
    "        #instance attributes\n",
    "        self.initial_investment = initial_investment\n",
    "        self.cur_step = None\n",
    "        self.pos_open = None #ex-stock_owned - max position opened at same time\n",
    "        self.buy_open = None #buy pos opened\n",
    "        self.sell_open = None #sell pos opened\n",
    "        self.buy_open_price = None\n",
    "        self.sell_open_price = None\n",
    "        self.cur_price = None\n",
    "        self.pips_cum = None\n",
    "        #experimental\n",
    "        self.prev_price = None\n",
    "        self.ask_price = None\n",
    "        self.cur_features = None\n",
    "        self.index = None\n",
    "        self.cash_in_hand = None\n",
    "        \n",
    "        self.action_space = np.arange(5**self.n_currencies)\n",
    "        \n",
    "        #action permutation\n",
    "        #returns a nested list with elements like:\n",
    "        #for 1 currency:\n",
    "        #[0]\n",
    "        #[1]\n",
    "        #[2]\n",
    "        #[2]\n",
    "        #etc.\n",
    "        # 0 = Buy\n",
    "        # 1 = Sell\n",
    "        # 2 = Hold\n",
    "        self.action_list = list(map(list, itertools.product([0,1,2,3,4], repeat=self.n_currencies)))\n",
    "        \n",
    "        #calculate size of state\n",
    "        self.state_dim = self.n_currencies*2+self.n_features+1+1 #self.n_features * 2 + 1\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.op_buy = {}\n",
    "        self.op_sell = {}\n",
    "        self.cur_step = 0\n",
    "        self.pos_open = np.zeros(self.n_currencies)\n",
    "        self.buy_open = 0\n",
    "        self.sell_open = 0\n",
    "        self.pips_cum = 0\n",
    "        self.cur_price = self.currency_price_history.Close.iloc[self.cur_step]\n",
    "        self.ask_price = self.currency_price_history.Ask.iloc[self.cur_step]\n",
    "        self.sell_open_price = 0.0\n",
    "        self.buy_open_price = 0.0\n",
    "        self.cur_features = self.currency_price_history.iloc[self.cur_step,1:]\n",
    "        self.cash_in_hand = self.initial_investment\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action,ops):\n",
    "        assert action in self.action_space\n",
    "        \n",
    "        #get current value before performin the action\n",
    "        prev_val = self._get_val()\n",
    "#         print(self.cur_step)\n",
    "        #update price, i.e. go to next day or candle\n",
    "        self.cur_step += 1\n",
    "\n",
    "        self.cur_price = self.currency_price_history.Close.iloc[self.cur_step]\n",
    "        self.ask_price = self.currency_price_history.Ask.iloc[self.cur_step]\n",
    "        #Current features-agregado por ppsev\n",
    "        self.cur_features = self.currency_price_history.iloc[self.cur_step,1:]\n",
    "        #Index para ver donde hace los trades -- experimental\n",
    "        self.index = self.currency_price_history.index[self.cur_step]\n",
    "        #perform a trade\n",
    "        self._trade(action,ops)\n",
    "        \n",
    "        #get the new value after taking the action\n",
    "        cur_val = self._get_val()\n",
    "        \n",
    "        #reward is the increase or decrease of the investemt - for now\n",
    "        #needs to change to sharpe ratio, kratio or some ratio\n",
    "        reward = cur_val - prev_val\n",
    "        \n",
    "        #done if we hve run out of data\n",
    "        done = self.cur_step == self.n_step-1\n",
    "\n",
    "        #store the current value of the portfolio here\n",
    "        info = {\"cur_val\": cur_val}\n",
    "        \n",
    "        #conform to the Gym API\n",
    "        return self._get_obs(), reward,done,info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        obs = np.empty(self.state_dim)\n",
    "        obs[:self.n_currencies] = self.buy_open #cantidad de buy abiertas\n",
    "        obs[self.n_currencies:self.n_currencies+1] = self.sell_open #cantidad de posiciones sell abiertas\n",
    "        obs[self.n_currencies+1:self.n_currencies+2] = self.cur_price #Precio actual del par de divisa\n",
    "        obs[self.n_currencies+2:-1] = self.cur_features #Current features\n",
    "        obs[-1] = self.buy_open*((self.cur_price-self.buy_open_price)/0.0001) +\\\n",
    "            self.sell_open*((self.sell_open_price-self.ask_price)/0.0001)  #los pips que hayan en el estado actual\n",
    "        return obs\n",
    "    \n",
    "    def _get_val(self):\n",
    "        return self.pips_cum #self.buy_open*((self.cur_price-self.buy_open_price)/0.0001) +\\\n",
    "            #self.sell_open*((self.sell_open_price-self.cur_price)/0.0001) \n",
    "\n",
    "    \n",
    "    def _trade(self, action,ops):\n",
    "        #index the action we want to perform\n",
    "        # 0 buy\n",
    "        # 1 sell\n",
    "        # 2 hold\n",
    "        #eg: [0] means buy currency\n",
    "        #eg: [0,1] means buy currency 1, sell currency 2\n",
    "        \n",
    "        action_vec = self.action_list[action]\n",
    "        \n",
    "        #determine currencies to buy and sell\n",
    "        sell_index = [] #stores index of currencies we want to sell\n",
    "        buy_index=[] #stores index of currenies we want to buy\n",
    "        close_buy_index = [] #stores index of currencies we want to close buy\n",
    "        close_sell_index = [] #stores index of currencies we want to close sell\n",
    "        \n",
    "        for i, a in enumerate(action_vec):\n",
    "            if a==0:\n",
    "                buy_index.append(i)\n",
    "            elif a == 1:\n",
    "                sell_index.append(i)\n",
    "            elif a == 2:\n",
    "                close_buy_index.append(i)\n",
    "            elif a == 3:\n",
    "                close_sell_index.append(i)\n",
    "        \n",
    "        #sell-buy when it corresponds\n",
    "        #we buy at ask price and close buys at bid/cur price.\n",
    "        #we sell at bid/cur price and close sells at ask price\n",
    "        if sell_index:\n",
    "            for i in sell_index:\n",
    "                if self.sell_open==0:\n",
    "                    self.sell_open_price = self.cur_price\n",
    "                    self.op_sell = {\"Magic_number\":0, \"Item\":\"EURUSD\", \"Type\":\"sell\",\"Open_Time\":self.index\\\n",
    "                              ,\"Open_Price\":self.cur_price}\n",
    "                    self.sell_open = 1\n",
    "\n",
    "        \n",
    "        if buy_index:\n",
    "            for i in buy_index:\n",
    "                if self.buy_open==0:\n",
    "                    self.buy_open_price=self.ask_price\n",
    "                    self.op_buy = {\"Magic_number\":0, \"Item\":\"EURUSD\", \"Type\":\"buy\",\"Open_Time\":self.index,\\\n",
    "                              \"Open_Price\":self.ask_price}\n",
    "                    self.buy_open = 1\n",
    "\n",
    "\n",
    "        if close_buy_index:\n",
    "            for i in close_buy_index:\n",
    "                if self.buy_open==1:\n",
    "#                     self.prev_price = self.cur_price\n",
    "                    self.op_buy[\"Close_Time\"] = self.index\n",
    "                    self.op_buy[\"Close_Price\"] = self.cur_price\n",
    "                    ops.append(self.op_buy)\n",
    "                    self.buy_open = 0\n",
    "                    self.pips_cum+= (self.cur_price-self.buy_open_price)/0.0001\n",
    "        \n",
    "        if close_sell_index:\n",
    "            for i in close_sell_index:\n",
    "                if self.sell_open==1:\n",
    "#                     self.prev_price = self.cur_price\n",
    "                    self.op_sell[\"Close_Time\"] = self.index\n",
    "                    self.op_sell[\"Close_Price\"] = self.ask_price\n",
    "                    ops.append(self.op_sell)\n",
    "                    self.sell_open = 0\n",
    "                    self.pips_cum+= (self.sell_open_price-self.ask_price)/0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(env,ops):\n",
    "    \"\"\"\n",
    "    Returns scikit-learn scaler object to scale the states\n",
    "    \"\"\"\n",
    "    states=[]\n",
    "    for _ in range(env.n_step):\n",
    "        action = np.random.choice(env.action_space)\n",
    "        state, reward, done, info = env.step(action,ops)\n",
    "        states.append(state)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(states)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the idea is to let the agent \"see\" not only the current value, but\n",
    "#previous values. Just like a normal person would do\n",
    "def create_window(data,window_size = 1):\n",
    "    data_s = pd.DataFrame(data.values,index=data.index)\n",
    "    data_c = data.copy()\n",
    "    for i in range(window_size):\n",
    "        data_c = pd.concat([data_c, data_s.shift((i + 1))], axis = 1)\n",
    "        \n",
    "    data_c.dropna(axis=0, inplace=True)\n",
    "    return(data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_EDGE = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_shape = env._get_obs()\n",
    "space_shape = np.reshape(space_shape,(1,space_shape.shape[0])).shape\n",
    "action_size = len(env.action_space)\n",
    "\n",
    "space_shape,action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gamma = .9\n",
    "test_batch_size = 128\n",
    "test_learning_rate = .0001\n",
    "test_hidden_neurons = 300\n",
    "test_critic_weight = 0.5\n",
    "test_entropy = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops=[]\n",
    "scaler = get_scaler(env,ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_memory = Memory(test_gamma, test_batch_size)\n",
    "test_actor, test_critic, test_policy = build_networks(\n",
    "    space_shape, action_size,\n",
    "    test_learning_rate, test_critic_weight,\n",
    "    test_hidden_neurons, test_entropy)\n",
    "test_agent = Agent(\n",
    "    test_actor, test_critic, test_policy, test_memory, action_size)\n",
    "\n",
    "rew = []\n",
    "\n",
    "for episode in range(2000):  \n",
    "    state = env.reset()\n",
    "    state = scaler.transform([state])\n",
    "    state = np.reshape(state,(1,state.shape[1]))\n",
    "    episode_reward = 0\n",
    "    ops=[]\n",
    "    done = False\n",
    "    \n",
    "    t0 = datetime.now()\n",
    "    \n",
    "    while not done:\n",
    "        action = test_agent.act(state)\n",
    "        state_prime, reward, done, _ = env.step(action,ops)\n",
    "        episode_reward += reward\n",
    "\n",
    "        state_prime = scaler.transform([state_prime])\n",
    "        next_value = test_agent.critic.predict([[state_prime]]) \n",
    "        test_agent.memory.add((state, action, reward, done, next_value))\n",
    "        state = state_prime\n",
    "    test_agent.learn()\n",
    "    t1 = datetime.now()-t0\n",
    "    rew.append(episode_reward)\n",
    "    total_buy=len(pd.DataFrame(ops)[pd.DataFrame(ops).Type==\"buy\"]) if len(ops) > 0 else 0\n",
    "    total_sell=len(pd.DataFrame(ops)[pd.DataFrame(ops).Type==\"sell\"]) if len(ops) > 0 else 0\n",
    "    print(\"Episode\", episode, \"Elapsed: \",t1,\", Total buy:\",total_buy,\", Total sell:\",total_sell,\n",
    "          \", pips acum:\",np.round(env.pips_cum,decimals=4),\", Score =\",np.round(episode_reward,decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
